{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e4f5945",
   "metadata": {},
   "source": [
    "# Section2：AlphaGo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316df806",
   "metadata": {},
   "source": [
    "## AlphaGoとは\n",
    "Google DeepMindによって開発されたコンピュータ囲碁プログラムである。\n",
    "コンピュータが人間に打ち勝つことが最も難しいと考えられてきた分野である囲碁において、人工知能が勝利を収めたことは、単なる一競技の勝敗を越え、人工知能の有用性を広く知らしめるものとなり、世界的AIブームを呼び起こすきっかけともなった。\n",
    "\n",
    "引用：[Wikipedia](https://ja.wikipedia.org/wiki/AlphaGo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2196f66",
   "metadata": {},
   "source": [
    "## Alpha Go (Lee)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0935bb",
   "metadata": {},
   "source": [
    "### PolicyNet(方策関数)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d252a79",
   "metadata": {},
   "source": [
    "### ValueNet(価値関数)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dd4647",
   "metadata": {},
   "source": [
    "### RollOutPolicy\n",
    "\n",
    "- NNではなく線形の方策関数\n",
    "- 探索中に高速に着手確率を出すために使用される"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb5cc0",
   "metadata": {},
   "source": [
    "### PolicyNetの教師あり学習\n",
    "- 3000万局面分の棋譜データを教師データとし、教師と同じ着手を予測できるよう学習を行った。\n",
    "- 具体的には、教師が着手した手を1とし残りを0とした19×19次元の配列を教師とし、それを分類問題として学習した。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47f9af1",
   "metadata": {},
   "source": [
    "### PolicyNetの強化学習\n",
    "\n",
    "- PolicyPoolとは、PolicyNetの強化学習の過程を500Iteraionごとに記録し保存しておいたものである。\n",
    "- 現状のPolicyNetとPolicyPoolからランダムに選択されたPolicyNetと対局シミュレーションを行い、その結果を用いて方策勾配法で学習を行った。\n",
    "- 現状のPolicyNet同士の対局ではなく、PolicyPoolに保存されているものとの対局を使用する理由は、対局に幅を持たせて過学習を防ごうというのが主である。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee104619",
   "metadata": {},
   "source": [
    "### ValueNetの学習\n",
    "- PolicyNetを使用して対局シミュレーションを行い、その結果の勝敗を教師として学習した。\n",
    "- 教師データ作成の手順は\n",
    " 1. SL PolicyNet(教師あり学習で作成したPolicyNet)でN手まで打つ。\n",
    " 1. N+1手目の手をランダムに選択し、その手で進めた局面をS（N+1）とする。\n",
    " 1. S（N+1）からRL PolicyNet（強化学習で作成したPolicyNet）で終局まで打ち、その勝敗報酬をRとする。\n",
    "- S(N+1）とRを教師データ対とし、損失関数を平均二乗誤差とし、回帰問題として学習した。\n",
    "- N手までとN+1手からのPolicyNetを別々にしてある理由は、過学習を防ぐためであると論文では説明されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f037eb15",
   "metadata": {},
   "source": [
    "## AlphaGo (Lee) とAlphaGo Zeroの違い\n",
    "1. 教師あり学習を一切行わず、強化学習のみで作成\n",
    "1. 特徴入力からヒューリスティックな要素を排除し、石の配置のみにした\n",
    "1. PolicyNetとValueNetを１つのネットワークに統合した\n",
    "1. Residual Net（後述）を導入した\n",
    "1. モンテカルロ木探索からRollOutシミュレーションをなくした"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76133c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1c6bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c284728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
