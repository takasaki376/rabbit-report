{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ca20ca",
   "metadata": {},
   "source": [
    "# Section1：強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57ec85",
   "metadata": {},
   "source": [
    "## 1-1 強化学習とは\n",
    "\n",
    "長期的に報酬を最大化できるように環境のなかで行動を選択できるエージェントを作ることを目標とする機械学習の一分野 \\\n",
    "　⇒　行動の結果として与えられる利益(報酬)をもとに、行動を決定する原理を改善していく仕組みです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e32f49e",
   "metadata": {},
   "source": [
    "## 1-2 強化学習の応用例\n",
    "\n",
    "### マーケティングの場合\n",
    "強化学習を職場の例での表現\n",
    "\n",
    "| 登場人物 | 行動 |\n",
    "| :---: | :--- |\n",
    "| 環境 |　会社の販売促進部 |\n",
    "| エージェント | プロフィールと購入履歴に基づいて、キャンペーンメールを送る顧客を決めるソフトウェアである。 |\n",
    "| 行動 |　顧客ごとに送信、非送信のふたつの行動を選ぶことになる。|\n",
    "| 報酬 | キャンペーンのコストという負の報酬とキャンペーンで生み出されると推測される売上という正の報酬を受ける|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943781c",
   "metadata": {},
   "source": [
    "## 1-3 探索と利用のトレードオフ\n",
    "- 環境について事前に完璧な知識があれば、最適な行動を予測し決定することは可能。　\\\n",
    "　⇒　どのような顧客にキャンペーンメールを送信すると、どのような行動を行うのかが既知である状況。 \\\n",
    " 　⇒　強化学習の場合、上記仮定は成り立たないとする。不完全な知識を元に行動しながら、データを収集。最適な行動を見つけていく。\n",
    "  \n",
    "1. 過去のデータで、ベストとされる行動のみを常に取り続ければ他にもっとベストな行動を見つけることはできない。 \\\n",
    "　⇒　探索が足りない\n",
    "1. 未知の行動のみを常に取り続ければ、過去の経験が活かせない。 \\\n",
    "　⇒　利用が足りない\n",
    "\n",
    "1と2はトレードオフ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ed612",
   "metadata": {},
   "source": [
    "## 1-4 強化学習のイメージ\n",
    "\n",
    "1. エージェント\n",
    " - 方策：何をやったら価値が高くなるか考えて行動する。\n",
    "1. 環境\n",
    " - 状態：環境の状態で、状況に応じて変わる内容\n",
    "1. エージェント\n",
    " - 価値：行動の結果、報酬を得られる。\n",
    " \n",
    "エージェントの方策として、良い方策を考えるのが強化学習"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d58e6",
   "metadata": {},
   "source": [
    "## 1-5 強化学習の差分\n",
    "\n",
    "### 強化学習と通常の教師あり、教師なし学習との違い\n",
    "結論:目標が違う\n",
    "- 教師なし、あり学習では、データに含まれるパターンを見つけ出すおよびそのデータから予測することが目標\n",
    "- 強化学習では、優れた方策を見つけることが目標\n",
    "\n",
    "### 強化学習の歴史\n",
    "#### 強化学習について\n",
    "- 冬の時代があったが、計算速度の進展により大規模な状態をもつ場合の、強化学習を可能としつつある。\n",
    "- 関数近似法と、Q学習を組み合わせる手法の登場\n",
    "\n",
    "#### Q学習\n",
    "・行動価値関数を、行動する毎に更新することにより学習を進める方法\n",
    "\n",
    "#### 関数近似法\n",
    "・価値関数や方策関数を関数近似する手法のこと"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa998d4",
   "metadata": {},
   "source": [
    "## 1-6 価値関数\n",
    "\n",
    "価値関数とは\n",
    "・価値を表す関数としては、状態価値関数と行動価値関数の2種類がある\n",
    "\n",
    "1. 状態価値関数：ある状態の価値に注目する\n",
    "1. 行動価値関数：状態と価値を組み合わせた価値に注目する\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dd53bb",
   "metadata": {},
   "source": [
    "## 1-7 方策関数\n",
    "\n",
    "方策関数とはエージェントが、どんな行動をするのかを決める関数である。 \\\n",
    "方策ベースの強化学習手法において、ある状態でどのような行動を採るのかの確率を与える関数のこと \\\n",
    "価値関数の結果を最大化するように学習する。\n",
    "\n",
    "$\\pi_{\\theta}(a|s)$：エージェントが取る行動の確率(方策関数) \\\n",
    "$V^{\\pi}(s)$：ある状態でから得られる報酬(状態価値関数) \\\n",
    "$Q^{\\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数) \\\n",
    "$\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$：ある行動をとる時の報酬\n",
    "\n",
    "### 方策反復法\n",
    "- 方策をモデル化して最適化する手法 \\\n",
    "　⇒　方策勾配法 \\\n",
    "\n",
    "\\begin{aligned}\n",
    "\\theta^{(t+1)} = \\theta^{(t)} + \\epsilon \\nabla J(\\theta)\n",
    "\\end{aligned}\n",
    "\n",
    "$t$：時間\n",
    "$\\theta$：重み\n",
    "$\\epsilon$：学習率\n",
    "$J$：誤差関数\n",
    "\n",
    "\n",
    "$J$とは？ \\\n",
    "　⇒　方策の良さ \\\n",
    "　　　定義しなければならない"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd96c65",
   "metadata": {},
   "source": [
    "## 1-8 方策方程式\n",
    "定義方法\n",
    "- 平均報酬\n",
    "- 割引報酬和\n",
    "\n",
    "上記の定義に対応して、行動価値関数:Q(s,a)の定義を行い、方策勾配定理が成り立つ。\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\pi_{\\theta}}[(\\nabla_{\\theta} log\\pi_{\\theta} (a|s) Q^{\\pi}(s,a))]\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "$\\pi_{\\theta}(a|s)$：エージェントが取る行動の確率(方策関数) \\\n",
    "$Q^{\\pi}(s,a)$：ある状態で取ったある行動から得られる報酬(行動価値関数) \\\n",
    "$\\pi_{\\theta}(a|s)Q^{\\pi}(s,a)$：ある行動をとる時の報\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69102755",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
