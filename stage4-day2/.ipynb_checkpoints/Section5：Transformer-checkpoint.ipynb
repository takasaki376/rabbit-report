{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be179538",
   "metadata": {},
   "source": [
    "# Section5：Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95b0c92",
   "metadata": {},
   "source": [
    "## ニューラル機械翻訳の問題点：長さに弱い\n",
    "- 翻訳元の文の内容をひとつのベクトルで表現 \n",
    "- 文長が長くなると表現力が足りなくなる \n",
    "- 文長と翻訳精度の関係性\n",
    "\n",
    "## Attention (注意機構) \n",
    "- 翻訳先の各単語を選択する際に、翻訳元の文中の各単語の隠れ状態を利用\n",
    "\n",
    "#### 翻訳元の各単語の隠れ状態の加重平均\n",
    "\n",
    "\\begin{aligned}\n",
    "C_i = \\sum_{j=1}^{T_x} \\alpha_{ij}h_j\n",
    "\\end{aligned}\n",
    "\n",
    "#### 重み(全て足すと１) 重みはFFNNで求める\n",
    "\n",
    "\\begin{aligned}\n",
    "\\alpha_{ij} &= \\frac{exp(e_{ij})}{\\sum_{k=1}^{T_x} exp(e_{ik})} \\\\[10px]\n",
    "e_{ik} &= a(s_{i-1} , h_j)\n",
    "\\end{aligned}\n",
    "\n",
    "### Attentionは何をしているのか\n",
    "query(検索クエリ)に一致するkeyを索引し、対応するvalueを取り出す操作であると見做すことができる。これは辞書オブジェクトの機能と同じである。\n",
    "\n",
    "## 注意機構には二種類ある\n",
    "### Source Target-Attention(ソースターゲット注意機構)\n",
    "\n",
    "### Self-Attention(自己注意機構)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e04f0b",
   "metadata": {},
   "source": [
    "## Transformer-Encoder\n",
    "\n",
    "### Scaled dot product attention\n",
    "\n",
    "\n",
    "\n",
    "### Position-Wise Feed-Forward Networks\n",
    "- 位置情報を保持したまま順伝播させる\n",
    "\n",
    "### Multi-Head attention\n",
    "- 全単語に関するAttentionをまとめて計算する\n",
    "\n",
    "#### 重みパラメタの異なる８個のヘッドを使用\n",
    "- ８個のScaled Dot-Product Attentionの出力をConcat \n",
    "- それぞれのヘッドが異なる種類の情報を収集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708fcdc0",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "- Encoderと同じく6層 \n",
    " - 各層で二種類の注意機構 \n",
    " - 注意機構の仕組みはEncoderとほぼ同じ \n",
    "- 自己注意機構 \n",
    " - 生成単語列の情報を収集 \n",
    " - 直下の層の出力へのアテンション \n",
    " - 未来の情報を見ないようにマスク \n",
    "- Encoder-Decoder attention \n",
    " - 入力文の情報を収集 \n",
    " - Encoderの出力へのアテンション\n",
    " \n",
    "### Add (Residual Connection) \n",
    "- 入出力の差分を学習させる \n",
    "- 実装上は出力に入力をそのまま加算するだけ \n",
    "- 効果：学習・テストエラーの低減 \n",
    "\n",
    "### Norm (Layer Normalization) \n",
    "- 各層においてバイアスを除く活性化関数への入力を平均０、分散１に正則化 \n",
    "- 効果：学習の高速化\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a944a1d9",
   "metadata": {},
   "source": [
    "## Position Encoding\n",
    "RNNを用いないので単語列の語順情報を追加する必要がある"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406cd810",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea39143",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
