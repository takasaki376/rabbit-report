{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0073dd2",
   "metadata": {},
   "source": [
    "# Section3：軽量化・高速化技術"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ebcf8",
   "metadata": {},
   "source": [
    "# 分散深層学習とは\n",
    "- 深層学習は多くのデータを使⽤したり、パラメータ調整のために多くの時間を使⽤したりするため、⾼速な計算が求められる。\n",
    "- 複数の計算資源(ワーカー)を使⽤し、並列的にニューラルネットを構成することで、効率の良い学習を⾏いたい。\n",
    "- データ並列化、モデル並列化、GPUによる⾼速技術は不可⽋である。\n",
    "\n",
    "※毎年１０倍のデータ量・モデルの複雑になっていっている。計算量が１０倍ずつになっている。コンピューターは１８ヵ月で２倍の性能になっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fba88",
   "metadata": {},
   "source": [
    "# 3 高速化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2114e5",
   "metadata": {},
   "source": [
    "## 3.1 データ並列化\n",
    "- 親モデルを各ワーカーに⼦モデルとしてコピー（ワーカーがコンピューターにあたる）\n",
    "- データを分割し、各ワーカーごとに計算させる\n",
    "- データ並列化は各モデルのパラメータの合わせ⽅で、同期型か⾮同期型か決まる。\n",
    "\n",
    "### データ並列化: 同期型\n",
    "パラメータ更新の流れ：各ワーカーが計算が終わるのを待ち、全ワーカーの勾配が出たところで勾配の平均を計算し、親モデルのパラメータを更新する。\n",
    "\n",
    "\n",
    "### データ並列化: ⾮同期型\n",
    "パラメータ更新の流れ：各ワーカーはお互いの計算を待たず、各子モデルごとに更新を行う。学習が終わった子モデルはパラメータサーバにPushされる。新たに学習を始める時は、パラメータサーバからPopしたモデルに対して学習していく。\n",
    "\n",
    "### 同期型と⾮同期型の⽐較\n",
    "- 処理のスピードは、お互いのワーカーの計算を待たない⾮同期型の⽅が早い。\n",
    "- ⾮同期型は最新のモデルのパラメータを利⽤できないので、学習が不安定になりやすい。\n",
    " - ⇒Stale Gradient Problem\n",
    "- 現在は同期型の⽅が精度が良いことが多いので、主流となっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed37aa",
   "metadata": {},
   "source": [
    "## 3.2 モデル並列化\n",
    "- 親モデルを各ワーカーに分割し、それぞれのモデルを学習させる。全てのデータで学習が終わった後で、⼀つのモデルに復元。\n",
    "- モデルが⼤きい時はモデル並列化を、データが⼤きい時はデータ並列化をすると良い。\n",
    "- モデルのパラメータ数が多いほど、スピードアップの効率も向上する。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a048d69e",
   "metadata": {},
   "source": [
    "## 3.3 GPUによる⾼速化\n",
    "-  GPGPU (General-purpose on GPU)\n",
    " - 元々の使⽤⽬的であるグラフィック以外の⽤途で使⽤されるGPUの総称\n",
    "- CPU\n",
    " - ⾼性能なコアが少数\n",
    " - 複雑で連続的な処理が得意\n",
    "- GPU\n",
    " - ⽐較的低性能なコアが多数\n",
    " - 簡単な並列処理が得意\n",
    " - ニューラルネットの学習は単純な⾏列演算が多いので、⾼速化が可能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9632fb",
   "metadata": {},
   "source": [
    "## 3.4 モデルの軽量化\n",
    "モデルの精度を維持しつつパラメータや演算回数を低減する⼿法の総称\n",
    "- ⾼メモリ負荷 ⾼い演算性能が求められる 通常は\n",
    "- 低メモリ 低演算性能での利⽤が必要とされるIotなど\n",
    "\n",
    "代表的な⼿法 として下記の 3 つがある\n",
    "- 量⼦化\n",
    "- 蒸留\n",
    "- プルーニング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c49de",
   "metadata": {},
   "source": [
    "### 3.4.1 量⼦化\n",
    "- ネットワークが⼤きくなると⼤量のパラメータが必要なり学習や推論に多くのメモリと演算処理が必要\n",
    " - 通常のパラメータの 64 bit 浮動⼩数点を 32 bit など下位の精度に落とすことでメモリと演算処理の削減を⾏う\n",
    "\n",
    "#### 利点\n",
    "- 計算の⾼速化\n",
    " - 倍精度演算(64 bit)と単精度演算(32 bit)は演算性能が⼤きく違うため、量⼦化により精度を落とすことによりより多くの計算をすることができる。\n",
    " - 深層学習で⽤いられる NVIDIA 社製の GPU の性能は下記のようになる。\n",
    "- 省メモリ化\n",
    " - ニューロンの重みを浮動小数点のbitを少なくし有効桁数を下げることで、ニューロンのメモリサイズを小さくすることができ、メモリ使用量を抑えることができる。\n",
    "\n",
    "#### ⽋点\n",
    "- 精度の低下\n",
    " - ニューロンが表現できる少数の有効桁数が小さくなるため"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fdeb1",
   "metadata": {},
   "source": [
    "### 3.4.2 蒸留\n",
    "\n",
    "#### モデルの簡約化\n",
    "学習済みの精度の⾼いモデルの知識を軽量なモデルへ継承させる。知識の継承により、軽量でありながら複雑なモデルに匹敵する精度のモデルを得ることが期待できる。\n",
    "\n",
    "#### 教師モデルと⽣徒モデル\n",
    "- 蒸留は教師モデルと⽣徒モデルの2つで構成される\n",
    "- 教師モデルの重みを固定し⽣徒モデルの重みを更新していく誤差は教師モデルと⽣徒モデルのそれぞれの誤差を使い重みを更新していく\n",
    "\n",
    "##### 教師モデル\n",
    "予測精度の⾼い、複雑なモデルやアンサンブルされたモデル\n",
    "\n",
    "##### ⽣徒モデル\n",
    "教師モデルをもとに作られる軽量なモデル\n",
    "\n",
    "#### 蒸留の利点\n",
    "蒸留によって少ない学習回数でより精度の良いモデルを作成することができている"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3868c061",
   "metadata": {},
   "source": [
    "### 3.4.3 プルーニング\n",
    "\n",
    "### 計算の⾼速化\n",
    "寄与の少ないニューロンの削減を行い、モデルの圧縮を行うことで高速に計算できる。\n",
    "\n",
    "ニューロンの削減の⼿法は重みが閾値以下の場合ニューロンを削減し、再学習を⾏う。 \\\n",
    "例として、重みが 0.1 以下のニューロンを削減する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff127830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
