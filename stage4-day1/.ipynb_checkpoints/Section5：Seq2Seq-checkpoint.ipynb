{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3051421",
   "metadata": {},
   "source": [
    "# Section5：Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737ea3c",
   "metadata": {},
   "source": [
    "Seq2seqとは? \\\n",
    "　⇒　Encoder-Decoderモデルの一種を指します。\n",
    "\n",
    "Seq2seqの具体的な用途とは? \\\n",
    "　⇒　機械対話や、機械翻訳などに使用されている。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc00d7fd",
   "metadata": {},
   "source": [
    "## 5.1 Encoder RNN\n",
    "ユーザーがインプットしたテキストデータを、単語等のトークンに区切って渡す構造。\n",
    "\n",
    "Taking　　　：文章を単語等のトークン毎に分割し、トークンごとのIDに分割する。 \\\n",
    "Embedding　 ：IDから、そのトークンを表す分散表現ベクトルに変換。 \\\n",
    "Encoder RNN ：ベクトルを順番にRNNに入力していく。\n",
    "\n",
    "- vec1をRNNに入力し、hidden stateを出力。このhidden stateと次の入力vec2をまたRNNに入力してきたhidden stateを出力という流れを繰り返す。\n",
    "- 最後のvecを入れたときのhidden stateをfinal stateとしてとっておく。このfinal stateがthought vectorと呼ばれ、入力した文の意味を表すベクトルとなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d8c50f",
   "metadata": {},
   "source": [
    "## 5.2 Decoder RNN\n",
    "システムがアウトプットデータを、単語等のトークンごとに生成する構造。\n",
    "\n",
    "1. Decoder RNN：Encoder RNN の final state (thought vector) から、各token の生成確率を出力していきますfinal state を Decoder RNN のinitial state ととして設定し、Embedding を入力。\n",
    "1. Sampling: 生成確率にもとづいて token をランダムに選びます。\n",
    "1. Embedding: 2で選ばれた token を Embedding して Decoder RNN への次の入力とします。\n",
    "1. Detokenize: 1 -3 を繰り返し、 2で得られた token を文字列に直します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e4549",
   "metadata": {},
   "source": [
    "## 5.3 HRED\n",
    "\n",
    "- 課題 \\\n",
    "一問一答しかできない \\\n",
    "　⇒　問に対して文脈も何もなく、ただ応答が行われる続ける。 \\\n",
    "　　⇒　HRED\n",
    "  \n",
    "HREDとは？ \\\n",
    "　⇒　過去 n-1 個の発話から次の発話を生成する。 \\\n",
    "　　⇒Seq2seqでは、会話の文脈無視で、応答がなされたが、HREDでは、前の単語の流れに即して応答されるため、より人間らしい文章が生成される。\n",
    " \n",
    "　⇒　Seq2Seq+ Context RNN \\\n",
    "　　⇒　Context RNN: Encoder のまとめた各文章の系列をまとめて、これまでの会話コンテキスト全体を表すベクトルに変換する構造。 \\\n",
    "　　　⇒　過去の発話の履歴を加味した返答をできる。\n",
    "   \n",
    "HREDの課題\n",
    "　⇒　HRED は確率的な多様性が字面にしかなく、会話の「流れ」のような多様性が無い。 \\\n",
    "　　⇒　同じコンテキスト（発話リスト）を与えられても、答えの内容が毎回会話の流れとしては同じものしか出せない。 \\\n",
    "　⇒　HRED は短く情報量に乏しい答えをしがちである。 \\\n",
    "　　⇒　短いよくある答えを学ぶ傾向がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feffbb5",
   "metadata": {},
   "source": [
    "## 5.4 VHRED\n",
    "\n",
    "VHREDとは？ \\\n",
    "　⇒　HREDに、VAEの潜在変数の概念を追加したもの。 \\\n",
    "　　⇒　HREDの課題を、VAEの潜在変数の概念を追加することで解決した構造。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9849fddb",
   "metadata": {},
   "source": [
    "## 5.5 VAE\n",
    "### 5.5.1 オートエンコーダー\n",
    "- オートエンコーダとは？ \\\n",
    "　⇒　教師なし学習の一つ。 そのため学習時の入力データは訓練データのみで教師データは利用しない。 \\\n",
    "- オートエンコーダ具体例 \\\n",
    "　⇒　MNISTの場合、28x28の数字の画像を入れて、同じ画像を出力するニューラルネットワークということになります。 \\\n",
    " \n",
    "- オートエンコーダ構造説明 \\\n",
    "入力データから潜在変数zに変換するニューラルネットワークをEncoder逆に潜在変数zをインプットとして元画像を復元するニューラルネットワークをDecoder。\n",
    "\n",
    "- メリット \\\n",
    "次元削減が行えること。\n",
    "\n",
    "### 5.5.2 VAE\n",
    "\n",
    "通常のオートエンコーダーの場合、何かしら潜在変数zにデータを押し込めているものの、その構造がどのような状態かわからない。 \\\n",
    "　⇒　VAEは、データを潜在変数zの確率分布という構造に押し込めることを可能にします。 \\\n",
    "　　⇒　VAEはこの潜在変数zに確率分布z∼N(0,1)を仮定したもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae55a4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cacb39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3320d29",
   "metadata": {},
   "source": [
    "# 確認テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21ce82",
   "metadata": {},
   "source": [
    "#### P109\n",
    "問） \\\n",
    "下記の選択肢から、seq2seqについて説明しているものを選べ。\n",
    "\n",
    "（1）時刻に関して順方向と逆方向のRNNを構成し、それら2つの中間層表現を特徴量として利用するものである。\n",
    "\n",
    "（2）RNNを用いたEncoder-Decoderモデルの一種であり、機械翻訳などのモデルに使われる。\n",
    "\n",
    "（3）構文木などの木構造に対して、隣接単語から表現ベクトル（フレーズ）を作るという演算を再帰的に行い（重みは共通）、文全体の表現ベクトルを得るニューラルネットワークである。\n",
    "\n",
    "（4）RNNの一種であり、単純なRNNにおいて問題となる勾配消失問題をCECとゲートの概念を導入することで解決したものである。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb70da46",
   "metadata": {},
   "source": [
    "答）\n",
    "２\n",
    "\n",
    "１：双方向RNN \\\n",
    "３：RNN \\\n",
    "４：LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d00a99d",
   "metadata": {},
   "source": [
    "#### P119\n",
    "問） \\\n",
    "seq2seqとHRED、HREDとVHREDの違いを簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36f7f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba957b3c",
   "metadata": {},
   "source": [
    "#### P128\n",
    "問） \\\n",
    "VAEに関する下記の説明文中の空欄に当てはまる言葉を答えよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d13328",
   "metadata": {},
   "source": [
    "答） \\\n",
    "自己符号化器の潜在変数に「確率分布z∼N(0,1)」を導入したもの。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a7852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
