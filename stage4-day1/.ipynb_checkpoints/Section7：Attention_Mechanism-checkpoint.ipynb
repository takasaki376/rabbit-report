{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b46fa1dd",
   "metadata": {},
   "source": [
    "# Section7：Attention Mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce73e06f-0dc1-43f9-b468-2cec8a88687d",
   "metadata": {},
   "source": [
    "- seq2seqの課題 \\\n",
    "長い文章への対応が難しい。seq2seq では、2単語でも、100単語でも、固定次元ベクトルの中に入力しなければならない。\n",
    "\n",
    "- 解決策 \\\n",
    "文章が長くなるほどそのシーケンスの内部表現の次元も大きくなっていく、仕組みが必要になります。\n",
    "\n",
    "- Attention Mechanism \\\n",
    "　⇒　「入力と出力のどの単語が関連しているのか」の関連度を学習する仕組み。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7049a09e",
   "metadata": {},
   "source": [
    "#### ゼロから作るディープラーニング② 8.1 Attentionの仕組み\n",
    "Attentionというメカニズムによって、必要な情報だけに「注目」を向けさせることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79bb157",
   "metadata": {},
   "source": [
    "#### ゼロから作るディープラーニング② 8.2 Attention付きseq2seqの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "383b1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "from common.time_layers import *\n",
    "from seq2seq import Encoder, Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288ed61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightSum:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, a):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ar = a.reshape(N, T, 1)#.repeat(T, axis=1)\n",
    "        t = hs * ar\n",
    "        c = np.sum(t, axis=1)\n",
    "\n",
    "        self.cache = (hs, ar)\n",
    "        return c\n",
    "\n",
    "    def backward(self, dc):\n",
    "        hs, ar = self.cache\n",
    "        N, T, H = hs.shape\n",
    "        dt = dc.reshape(N, 1, H).repeat(T, axis=1)\n",
    "        dar = dt * hs\n",
    "        dhs = dt * ar\n",
    "        da = np.sum(dar, axis=2)\n",
    "\n",
    "        return dhs, da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e67021c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeight:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.softmax = Softmax()\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        hr = h.reshape(N, 1, H)#.repeat(T, axis=1)\n",
    "        t = hs * hr\n",
    "        s = np.sum(t, axis=2)\n",
    "        a = self.softmax.forward(s)\n",
    "\n",
    "        self.cache = (hs, hr)\n",
    "        return a\n",
    "\n",
    "    def backward(self, da):\n",
    "        hs, hr = self.cache\n",
    "        N, T, H = hs.shape\n",
    "\n",
    "        ds = self.softmax.backward(da)\n",
    "        dt = ds.reshape(N, T, 1).repeat(H, axis=2)\n",
    "        dhs = dt * hr\n",
    "        dhr = dt * hs\n",
    "        dh = np.sum(dhr, axis=1)\n",
    "\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58b93af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.attention_weight_layer = AttentionWeight()\n",
    "        self.weight_sum_layer = WeightSum()\n",
    "        self.attention_weight = None\n",
    "\n",
    "    def forward(self, hs, h):\n",
    "        a = self.attention_weight_layer.forward(hs, h)\n",
    "        out = self.weight_sum_layer.forward(hs, a)\n",
    "        self.attention_weight = a\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dhs0, da = self.weight_sum_layer.backward(dout)\n",
    "        dhs1, dh = self.attention_weight_layer.backward(da)\n",
    "        dhs = dhs0 + dhs1\n",
    "        return dhs, dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2694242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeAttention:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.layers = None\n",
    "        self.attention_weights = None\n",
    "\n",
    "    def forward(self, hs_enc, hs_dec):\n",
    "        N, T, H = hs_dec.shape\n",
    "        out = np.empty_like(hs_dec)\n",
    "        self.layers = []\n",
    "        self.attention_weights = []\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = Attention()\n",
    "            out[:, t, :] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
    "            self.layers.append(layer)\n",
    "            self.attention_weights.append(layer.attention_weight)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        N, T, H = dout.shape\n",
    "        dhs_enc = 0\n",
    "        dhs_dec = np.empty_like(dout)\n",
    "\n",
    "        for t in range(T):\n",
    "            layer = self.layers[t]\n",
    "            dhs, dh = layer.backward(dout[:, t, :])\n",
    "            dhs_enc += dhs\n",
    "            dhs_dec[:,t,:] = dh\n",
    "\n",
    "        return dhs_enc, dhs_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd2b86e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionEncoder(Encoder):\n",
    "    def forward(self, xs):\n",
    "        xs = self.embed.forward(xs)\n",
    "        hs = self.lstm.forward(xs)\n",
    "        return hs\n",
    "\n",
    "    def backward(self, dhs):\n",
    "        dout = self.lstm.backward(dhs)\n",
    "        dout = self.embed.backward(dout)\n",
    "        return dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d92776",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.attention = TimeAttention()\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in layers:\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "\n",
    "    def forward(self, xs, enc_hs):\n",
    "        h = enc_hs[:,-1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        dec_hs = self.lstm.forward(out)\n",
    "        c = self.attention.forward(enc_hs, dec_hs)\n",
    "        out = np.concatenate((c, dec_hs), axis=2)\n",
    "        score = self.affine.forward(out)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        dout = self.affine.backward(dscore)\n",
    "        N, T, H2 = dout.shape\n",
    "        H = H2 // 2\n",
    "\n",
    "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
    "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
    "        ddec_hs = ddec_hs0 + ddec_hs1\n",
    "        dout = self.lstm.backward(ddec_hs)\n",
    "        dh = self.lstm.dh\n",
    "        denc_hs[:, -1] += dh\n",
    "        self.embed.backward(dout)\n",
    "\n",
    "        return denc_hs\n",
    "\n",
    "    def generate(self, enc_hs, start_id, sample_size):\n",
    "        sampled = []\n",
    "        sample_id = start_id\n",
    "        h = enc_hs[:, -1]\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([sample_id]).reshape((1, 1))\n",
    "\n",
    "            out = self.embed.forward(x)\n",
    "            dec_hs = self.lstm.forward(out)\n",
    "            c = self.attention.forward(enc_hs, dec_hs)\n",
    "            out = np.concatenate((c, dec_hs), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            sample_id = np.argmax(score.flatten())\n",
    "            sampled.append(sample_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddee6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionSeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        args = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = AttentionEncoder(*args)\n",
    "        self.decoder = AttentionDecoder(*args)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "255ea207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    '''\n",
    "    Adam (http://arxiv.org/abs/1412.6980v8)\n",
    "    '''\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        \n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = [], []\n",
    "            for param in params:\n",
    "                self.m.append(np.zeros_like(param))\n",
    "                self.v.append(np.zeros_like(param))\n",
    "        \n",
    "        self.iter += 1\n",
    "        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        for i in range(len(params)):\n",
    "            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])\n",
    "            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])\n",
    "            \n",
    "            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88d02fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import sequence\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af95814f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 0[s] | loss 4.08\n",
      "| epoch 1 |  iter 101 / 351 | time 32[s] | loss 1.87\n",
      "| epoch 1 |  iter 201 / 351 | time 64[s] | loss 1.07\n",
      "| epoch 1 |  iter 301 / 351 | time 96[s] | loss 1.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "X 1978-08-11\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "X 1978-08-11\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "X 1978-08-11\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 1978-08-11\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1978-08-11\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 1978-08-11\n",
      "---\n",
      "val acc 0.000%\n",
      "| epoch 2 |  iter 1 / 351 | time 0[s] | loss 1.00\n",
      "| epoch 2 |  iter 101 / 351 | time 33[s] | loss 0.99\n",
      "| epoch 2 |  iter 201 / 351 | time 66[s] | loss 0.97\n",
      "| epoch 2 |  iter 301 / 351 | time 98[s] | loss 0.81\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "X 2006-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "X 2007-08-09\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "X 1983-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "X 2016-11-08\n",
      "---\n",
      "val acc 51.020%\n",
      "| epoch 3 |  iter 1 / 351 | time 0[s] | loss 0.35\n",
      "| epoch 3 |  iter 101 / 351 | time 32[s] | loss 0.16\n",
      "| epoch 3 |  iter 201 / 351 | time 64[s] | loss 0.03\n",
      "| epoch 3 |  iter 301 / 351 | time 96[s] | loss 0.01\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.900%\n",
      "| epoch 4 |  iter 1 / 351 | time 0[s] | loss 0.01\n",
      "| epoch 4 |  iter 101 / 351 | time 32[s] | loss 0.01\n",
      "| epoch 4 |  iter 201 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 4 |  iter 301 / 351 | time 97[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.940%\n",
      "| epoch 5 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 5 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 5 |  iter 201 / 351 | time 65[s] | loss 0.00\n",
      "| epoch 5 |  iter 301 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| epoch 6 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 6 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 6 |  iter 201 / 351 | time 63[s] | loss 0.00\n",
      "| epoch 6 |  iter 301 / 351 | time 95[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| epoch 7 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 7 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 7 |  iter 201 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 7 |  iter 301 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 99.960%\n",
      "| epoch 8 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 8 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 8 |  iter 201 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 8 |  iter 301 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 9 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 9 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 9 |  iter 201 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 9 |  iter 301 / 351 | time 96[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n",
      "val acc 100.000%\n",
      "| epoch 10 |  iter 1 / 351 | time 0[s] | loss 0.00\n",
      "| epoch 10 |  iter 101 / 351 | time 32[s] | loss 0.00\n",
      "| epoch 10 |  iter 201 / 351 | time 64[s] | loss 0.00\n",
      "| epoch 10 |  iter 301 / 351 | time 97[s] | loss 0.00\n",
      "Q 10/15/94                     \n",
      "T 1994-10-15\n",
      "O 1994-10-15\n",
      "---\n",
      "Q thursday, november 13, 2008  \n",
      "T 2008-11-13\n",
      "O 2008-11-13\n",
      "---\n",
      "Q Mar 25, 2003                 \n",
      "T 2003-03-25\n",
      "O 2003-03-25\n",
      "---\n",
      "Q Tuesday, November 22, 2016   \n",
      "T 2016-11-22\n",
      "O 2016-11-22\n",
      "---\n",
      "Q Saturday, July 18, 1970      \n",
      "T 1970-07-18\n",
      "O 1970-07-18\n",
      "---\n",
      "Q october 6, 1992              \n",
      "T 1992-10-06\n",
      "O 1992-10-06\n",
      "---\n",
      "Q 8/23/08                      \n",
      "T 2008-08-23\n",
      "O 2008-08-23\n",
      "---\n",
      "Q 8/30/07                      \n",
      "T 2007-08-30\n",
      "O 2007-08-30\n",
      "---\n",
      "Q 10/28/13                     \n",
      "T 2013-10-28\n",
      "O 2013-10-28\n",
      "---\n",
      "Q sunday, november 6, 2016     \n",
      "T 2016-11-06\n",
      "O 2016-11-06\n",
      "---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc 100.000%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
    "char_to_id, id_to_char = sequence.get_vocab()\n",
    "\n",
    "# 入力文を反転\n",
    "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
    "\n",
    "# ハイパーパラメータの設定\n",
    "vocab_size = len(char_to_id)\n",
    "wordvec_size = 16\n",
    "hidden_size = 256\n",
    "batch_size = 128\n",
    "max_epoch = 10\n",
    "max_grad = 5.0\n",
    "\n",
    "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "# model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
    "\n",
    "optimizer = Adam()\n",
    "trainer = Trainer(model, optimizer)\n",
    "\n",
    "\n",
    "acc_list = []\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train, t_train, max_epoch=1,\n",
    "                batch_size=batch_size, max_grad=max_grad ,eval_interval=100)\n",
    "\n",
    "    correct_num = 0\n",
    "    for i in range(len(x_test)):\n",
    "        question, correct = x_test[[i]], t_test[[i]]\n",
    "        verbose = i < 10\n",
    "        correct_num += eval_seq2seq(model, question, correct,\n",
    "                                    id_to_char, verbose, is_reverse=True)\n",
    "\n",
    "    acc = float(correct_num) / len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc * 100))\n",
    "\n",
    "\n",
    "model.save_params()\n",
    "\n",
    "# グラフの描画\n",
    "x = np.arange(len(acc_list))\n",
    "plt.plot(x, acc_list, marker='o')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4600a25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b53bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ff61b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba451c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78dd04f9",
   "metadata": {},
   "source": [
    "# 確認テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a25ebf",
   "metadata": {},
   "source": [
    "#### P137\n",
    "問） \\\n",
    "RNNとword2vec、seq2seqとAttentionの違いを簡潔に述べよ。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7f097",
   "metadata": {},
   "source": [
    "答） \n",
    "- RNNとWord2vec \n",
    " - RNNは、時系列データを処理するのに適したネットワーク\n",
    " - Word2vecは、単語の分散表現ベクトルを得る手法\n",
    "- Seq2SeqとAttention \n",
    " - Seq2Seqは、ひとつの時系列データから別の時系列データを得るネットワーク\n",
    " - Attentionは、時系列データの中身に対して、関連性の重みをつける手法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e7439e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
